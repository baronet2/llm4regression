{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude 3 Opus predicted: 129.01\n",
      "Gold is: 146.8\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "## Step 1: Set up your API key\n",
    "##############################\n",
    "\n",
    "import os\n",
    "\n",
    "# This key will not be active by the time you see this :)\n",
    "os.environ['OPENROUTER_API_KEY'] = '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'\n",
    "\n",
    "###############################################\n",
    "## Step 2: create an llm object to call Claude 3 Opus \n",
    "###############################################\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from typing import Optional\n",
    "\n",
    "class ChatOpenRouter(ChatOpenAI):\n",
    "    \"\"\"\n",
    "    OpenRouter uses same API as OpenAI\n",
    "    See: https://medium.com/@gal.peretz/openrouter-langchain-leverage-opensource-models-without-the-ops-hassle-9ffbf0016da7\n",
    "    \"\"\"\n",
    "    openai_api_base: str\n",
    "    openai_api_key: str\n",
    "    model_name: str\n",
    "\n",
    "    def __init__(self,\n",
    "                 model_name: str,\n",
    "                 openai_api_key: Optional[str] = None,\n",
    "                 openai_api_base: str = \"https://openrouter.ai/api/v1\",\n",
    "                 **kwargs):\n",
    "        openai_api_key = openai_api_key or os.getenv('OPENROUTER_API_KEY')\n",
    "        super().__init__(openai_api_base=openai_api_base,\n",
    "                         openai_api_key=openai_api_key,\n",
    "                         model_name=model_name, **kwargs)\n",
    "\n",
    "llm = ChatOpenRouter(model_name='anthropic/claude-3-opus', temperature=0, max_retries=5)\n",
    "\n",
    "\n",
    "#############################\n",
    "## Step 3: Create the dataset\n",
    "#############################\n",
    "# Here, we will use Friedman #2\n",
    "from sklearn.datasets import make_friedman2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# The data from sklearn\n",
    "r_data, r_values = make_friedman2(n_samples=51, noise=0, random_state=1)\n",
    "\n",
    "# Create a dataframe; Not mandatory, but makes things easier\n",
    "df = pd.DataFrame({**{f'Feature {i}': r_data[:, i] for i in range(r_data.shape[1])}, 'Output': r_values})\n",
    "x = df.drop(['Output'], axis=1)\n",
    "y = df['Output']\n",
    "\n",
    "# Round the values to 2 decimal places\n",
    "# Not mandatory, but helps to: (1) Keep the costs low, (2) Work with the same numbers of examples with models that have a smaller context (e.g., Yi, Llama, etc)\n",
    "x = np.round(x, 2)\n",
    "y = np.round(y, 2)\n",
    "\n",
    "# Do a random split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=1, random_state=1)\n",
    "\n",
    "\n",
    "x_train = x_train.iloc[:50]\n",
    "y_train = y_train.iloc[:50]\n",
    "x_test  = x_test.iloc[:1]\n",
    "y_test  = y_test.iloc[:1]\n",
    "\n",
    "# By this point, the dataset is ready\n",
    "\n",
    "\n",
    "#############################\n",
    "## Step 4: Prepare the prompt\n",
    "#############################\n",
    "from langchain import PromptTemplate, FewShotPromptTemplate\n",
    "# The suffix for the FewShotPromptTemplate. For example:\n",
    "# ```\n",
    "# Feature 0: <number>\n",
    "# Feature 1: <number>\n",
    "# Output:\n",
    "# ```\n",
    "# This suffix is used for the test example\n",
    "suffix = [feature + \": {\" + f\"{feature}\" + \"}\" for feature in x_train.columns] + [y_train.name + \":\"]\n",
    "suffix = \"\\n\".join(suffix)\n",
    "\n",
    "input_variables=x_train.columns.to_list()\n",
    "\n",
    "# The template for the in-context examples. Here, you also give the expected output\n",
    "template = [feature + \": {\" + f\"{feature}\" + \"}\" for feature in x_train.columns] + [y_train.name + \": {\" + f\"{y_train.name}\" + \"}\"]\n",
    "template = \"\\n\".join(template)\n",
    "example_prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=x_train.columns.to_list() + [y_train.name],\n",
    ")\n",
    "\n",
    "\n",
    "# Create the few-shot prompt template\n",
    "fspt = FewShotPromptTemplate(\n",
    "    examples        =  [{**x1, y_train.name: x2} for x1, x2 in zip(x_train.to_dict('records'), y_train)],\n",
    "    example_prompt  =  example_prompt,\n",
    "    suffix          =  suffix,\n",
    "    input_variables = input_variables,\n",
    ")\n",
    "\n",
    "# An instruction to prevent the model from generating explanations.\n",
    "prefix_instruction = 'The task is to provide your best estimate for \"Output\". Please provide that and only that, without any additional text.\\n\\n\\n\\n\\n'\n",
    "\n",
    "# By this point, we have the (1) LLM, (2) Data, (3) Prompt; We are ready to run\n",
    "\n",
    "########################\n",
    "## Step 5: Run the model\n",
    "########################\n",
    "\n",
    "predicted = llm.call_as_llm(prefix_instruction + fspt.format(**x_test.to_dict('records')[0]))\n",
    "gold      = y_test.iloc[0].item()\n",
    "\n",
    "print(f\"Claude 3 Opus predicted: {predicted}\")\n",
    "print(f\"Gold is: {gold}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm4r_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
