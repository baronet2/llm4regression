{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/rvacareanu/apps/miniconda3/envs/llm4r_v2/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n",
      "/storage/rvacareanu/apps/miniconda3/envs/llm4r_v2/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `call_as_llm` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-4 predicted: 74.97\n",
      "Gold is: 146.8\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "## Step 1: Set up your API key\n",
    "##############################\n",
    "\n",
    "import os\n",
    "\n",
    "# This key will not be active by the time you see this :)\n",
    "os.environ['OPENAI_API_KEY'] = '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'\n",
    "\n",
    "###############################################\n",
    "## Step 2: create an llm object to call `gpt-4` \n",
    "###############################################\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI(model_name=\"gpt-4-0125-preview\", temperature=0)\n",
    "\n",
    "\n",
    "#############################\n",
    "## Step 3: Create the dataset\n",
    "#############################\n",
    "# Here, we will use Friedman #2\n",
    "from sklearn.datasets import make_friedman2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# The data from sklearn\n",
    "r_data, r_values = make_friedman2(n_samples=51, noise=0, random_state=1)\n",
    "\n",
    "# Create a dataframe; Not mandatory, but makes things easier\n",
    "df = pd.DataFrame({**{f'Feature {i}': r_data[:, i] for i in range(r_data.shape[1])}, 'Output': r_values})\n",
    "x = df.drop(['Output'], axis=1)\n",
    "y = df['Output']\n",
    "\n",
    "# Round the values to 2 decimal places\n",
    "# Not mandatory, but helps to: (1) Keep the costs low, (2) Work with the same numbers of examples with models that have a smaller context (e.g., Yi, Llama, etc)\n",
    "x = np.round(x, 2)\n",
    "y = np.round(y, 2)\n",
    "\n",
    "# Do a random split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=1, random_state=1)\n",
    "\n",
    "\n",
    "x_train = x_train.iloc[:50]\n",
    "y_train = y_train.iloc[:50]\n",
    "x_test  = x_test.iloc[:1]\n",
    "y_test  = y_test.iloc[:1]\n",
    "\n",
    "# By this point, the dataset is ready\n",
    "\n",
    "\n",
    "#############################\n",
    "## Step 4: Prepare the prompt\n",
    "#############################\n",
    "from langchain import PromptTemplate, FewShotPromptTemplate\n",
    "# The suffix for the FewShotPromptTemplate. For example:\n",
    "# ```\n",
    "# Feature 0: <number>\n",
    "# Feature 1: <number>\n",
    "# Output:\n",
    "# ```\n",
    "# This suffix is used for the test example\n",
    "suffix = [feature + \": {\" + f\"{feature}\" + \"}\" for feature in x_train.columns] + [y_train.name + \":\"]\n",
    "suffix = \"\\n\".join(suffix)\n",
    "\n",
    "input_variables=x_train.columns.to_list()\n",
    "\n",
    "# The template for the in-context examples. Here, you also give the expected output\n",
    "template = [feature + \": {\" + f\"{feature}\" + \"}\" for feature in x_train.columns] + [y_train.name + \": {\" + f\"{y_train.name}\" + \"}\"]\n",
    "template = \"\\n\".join(template)\n",
    "example_prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=x_train.columns.to_list() + [y_train.name],\n",
    ")\n",
    "\n",
    "\n",
    "# Create the few-shot prompt template\n",
    "fspt = FewShotPromptTemplate(\n",
    "    examples        =  [{**x1, y_train.name: x2} for x1, x2 in zip(x_train.to_dict('records'), y_train)],\n",
    "    example_prompt  =  example_prompt,\n",
    "    suffix          =  suffix,\n",
    "    input_variables = input_variables,\n",
    ")\n",
    "\n",
    "# An instruction to prevent the model from generating explanations.\n",
    "prefix_instruction = 'The task is to provide your best estimate for \"Output\". Please provide that and only that, without any additional text.\\n\\n\\n\\n\\n'\n",
    "\n",
    "# By this point, we have the (1) LLM, (2) Data, (3) Prompt; We are ready to run\n",
    "\n",
    "########################\n",
    "## Step 5: Run the model\n",
    "########################\n",
    "\n",
    "predicted = llm.call_as_llm(prefix_instruction + fspt.format(**x_test.to_dict('records')[0]))\n",
    "gold      = y_test.iloc[0].item()\n",
    "\n",
    "print(f\"GPT-4 predicted: {predicted}\")\n",
    "print(f\"Gold is: {gold}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm4r_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
